name: release-pullrequest-EKS-installer-chart

on:
  pull_request:
    paths:
      - '.github/workflows/release-pullrequest-EKS-installer-chart.yaml'
    branches:
      - main
  workflow_dispatch:

env:
  MODULE: installer
  GHCR_REPO: ghcr.io/${{ github.repository }}
  INSTALL_CRDS: "true"
  HAS_DEPLOYMENT: "true"

jobs:
  install-EKS:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        k8s_version: [ "1.33", "1.34" ]

    steps:
      - name: Authenticate with GitHub App
        id: authenticate
        uses: tibdex/github-app-token@v1
        with:
          app_id: ${{ secrets.APP_ID }}
          private_key: ${{ secrets.PRIVATE_KEY }}

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Extract version
        run: |
          set -euo pipefail
          git fetch --tags --force

          echo "=== TAGS IN REPO (sorted) ==="
          git tag -l --sort=-version:refname | cat

          echo
          echo "=== Searching latest tags for module '${{ env.MODULE }}' ==="
          APP_VERSION="$(
            git tag -l '${{ env.MODULE }}/*' --sort=-version:refname \
            | head -n1 || true
          )"
          echo "Raw APP_VERSION tag: '${APP_VERSION}'"

          CHART_VERSION="$(
            git tag -l '${{ env.MODULE }}-chart/*' --sort=-version:refname \
            | head -n1 || true
          )"
          echo "Raw CHART_VERSION tag: '${CHART_VERSION}'"

          echo
          echo "=== Applying fallbacks if empty ==="
          APP_VERSION="${APP_VERSION:-${{ env.MODULE }}/0.0.1}"
          CHART_VERSION="${CHART_VERSION:-${{ env.MODULE }}-chart/0.0.1}"
          echo "After fallback → APP_VERSION='${APP_VERSION}'"
          echo "After fallback → CHART_VERSION='${CHART_VERSION}'"

          echo
          echo "=== Stripping prefix (keep only part after last '/') ==="
          APP_VERSION="${APP_VERSION##*/}"
          CHART_VERSION="${CHART_VERSION##*/}"
          echo "After strip → APP_VERSION='${APP_VERSION}'"
          echo "After strip → CHART_VERSION='${CHART_VERSION}'"

          {
            echo "APP_VERSION=$APP_VERSION"
            echo "CHART_VERSION=$CHART_VERSION"
          } >> "$GITHUB_ENV"

      - name: Echo
        run: |
          echo "APP_VERSION=${{ env.APP_VERSION }}"
          echo "CHART_VERSION=${{ env.CHART_VERSION }}"

      - name: Replace CHART_VERSION in ./${{ env.MODULE }}-chart/Chart.yaml
        run: sed -i 's/CHART_VERSION/${{ env.CHART_VERSION }}/g' ./${{ env.MODULE }}-chart/Chart.yaml

      - name: Replace APP_VERSION in ./${{ env.MODULE }}-chart/Chart.yaml
        run: sed -i 's/APP_VERSION/${{ env.APP_VERSION }}/g' ./${{ env.MODULE }}-chart/Chart.yaml

      - name: Set up Helm
        uses: azure/setup-helm@v4.1.0

      - name: Helm lint chart
        run: helm lint ./${{ env.MODULE }}-chart

      - name: Create k8s Kind Cluster
        uses: helm/kind-action@v1

      # Prepare a sanitized version string (replace '.' with '-') and store it in K8S_VERSION_NODOT
      - name: Prepare version variable
        run: echo "K8S_VERSION_NODOT=$(echo '${{ matrix.k8s_version }}' | tr '.' '-')" >> $GITHUB_ENV

      - name: Install AWS Signing Helper
        run: |
          curl -Lo aws_signing_helper https://rolesanywhere.amazonaws.com/releases/1.4.0/X86_64/Linux/aws_signing_helper
          chmod +x aws_signing_helper

      - name: Write Certificate from Secrets
        run: |
          echo "${{ secrets.AWS_CERTIFICATE }}" | base64 --decode > certificato.txt
          chmod 600 certificato.txt

      - name: Write Private Key from Secrets
        run: |
          echo "${{ secrets.AWS_PRIVATE_KEY }}" | base64 --decode > chiave_privata_decifrata.key
          chmod 600 chiave_privata_decifrata.key

      - name: Get AWS Credentials via IAM Roles Anywhere
        run: |
          echo "Retrieving AWS credentials..."
          CREDS=$(./aws_signing_helper credential-process \
                --certificate ./certificato.txt \
                --private-key ./chiave_privata_decifrata.key \
                --trust-anchor-arn arn:aws:rolesanywhere:eu-central-1:654654440307:trust-anchor/5f810e29-6061-46d6-a1ff-d36a5ac5c8cc \
                --profile-arn arn:aws:rolesanywhere:eu-central-1:654654440307:profile/32097de8-0ad3-41f6-8da9-c99fa1c82bf5 \
                --role-arn arn:aws:iam::654654440307:role/github-action-admin \
                --session-duration 14400)

          ACCESS_KEY=$(echo $CREDS | jq -r '.AccessKeyId')
          SECRET_KEY=$(echo $CREDS | jq -r '.SecretAccessKey')
          SESSION_TOKEN=$(echo $CREDS | jq -r '.SessionToken')
          EXPIRATION=$(echo $CREDS | jq -r '.Expiration')

          # Export to GITHUB_ENV
          echo "AWS_ACCESS_KEY_ID=$ACCESS_KEY" >> "$GITHUB_ENV"
          echo "AWS_SECRET_ACCESS_KEY=$SECRET_KEY" >> "$GITHUB_ENV"
          echo "AWS_SESSION_TOKEN=$SESSION_TOKEN" >> "$GITHUB_ENV"
          echo "AWS_DEFAULT_REGION=eu-central-1" >> "$GITHUB_ENV"

          cat > ./credentials <<EOF
          [dev]
          aws_access_key_id = $ACCESS_KEY
          aws_secret_access_key = $SECRET_KEY
          aws_session_token = $SESSION_TOKEN
          EOF

          echo "AWS credentials retrieved. Session expires at: $EXPIRATION"

      - name: Create AWS Credentials Secret in Kubernetes
        run: |
          kubectl create ns ack-system
          kubectl create secret generic aws-creds \
            --from-file=credentials-file=./credentials \
            --namespace=ack-system \
            --type=Opaque

      - name: Install ACK EC2 Controller on Kind using Helm
        run: |
          helm install --create-namespace -n ack-system ack-ec2-controller \
          oci://public.ecr.aws/aws-controllers-k8s/ec2-chart --version=1.9.0 --set=aws.region=eu-central-1 --wait --set=aws.credentials.secretName=aws-creds --set=aws.credentials.secretKey=credentials-file --set=aws.credentials.profile=dev

      - name: Annotate ACK EC2 Controller serviceaccount
        run: kubectl annotate serviceaccount -n ack-system ack-ec2-controller eks.amazonaws.com/role-arn=arn:aws:iam::539247454863:role/github-action

      - name: Check ACK EC2 Controller serviceaccount
        run: kubectl describe serviceaccount -n ack-system ack-ec2-controller

      - name: Restart ACK EC2 Controller 
        run: kubectl rollout restart deployment -n ack-system ack-ec2-controller-ec2-chart  

      - name: Wait for ACK EC2 Controller Deployment to be Ready
        run:  kubectl rollout status deployment -n ack-system ack-ec2-controller-ec2-chart --timeout=300s

      - name: Install ACK EKS Controller on Kind using Helm
        run: |
          helm install --create-namespace -n ack-system ack-eks-controller \
          oci://public.ecr.aws/aws-controllers-k8s/eks-chart --version=1.11.0 --set=aws.region=eu-central-1 --wait --set=aws.credentials.secretName=aws-creds --set=aws.credentials.secretKey=credentials-file --set=aws.credentials.profile=dev

      - name: Annotate ACK EKS Controller serviceaccount
        run: kubectl annotate serviceaccount -n ack-system ack-eks-controller eks.amazonaws.com/role-arn=arn:aws:iam::539247454863:role/github-action

      - name: Check ACK EKS Controller serviceaccount
        run: kubectl describe serviceaccount -n ack-system ack-eks-controller

      - name: Restart ACK EKS Controller 
        run: kubectl rollout restart deployment -n ack-system ack-eks-controller-eks-chart  

      - name: Wait for ACK EKS Controller Deployment to be Ready
        run:  kubectl rollout status deployment -n ack-system ack-eks-controller-eks-chart --timeout=300s

      - name: Deploy VPC CR via EC2 Controller
        run: |
          echo "Deploying VPC resource with Kubernetes version ${{ matrix.k8s_version }}..."
          cat <<EOF | kubectl apply -f -
          apiVersion: ec2.services.k8s.aws/v1alpha1
          kind: VPC
          metadata:
            name: cluster-$K8S_VERSION_NODOT
          spec:
            cidrBlocks: 
              - 10.0.0.0/16
            enableDNSSupport: true
            enableDNSHostnames: true
          EOF

      - name: Wait for VPC Provisioning
        run: |
          echo "Waiting for VPC resource to become ACK.ResourceSynced..."
          kubectl wait --for=condition=ACK.ResourceSynced --timeout=600s vpc/cluster-$K8S_VERSION_NODOT

      - name: Check VPC status
        if: always()
        run: |
          kubectl get vpc/cluster-$K8S_VERSION_NODOT -o yaml

      - name: Create Internet Gateway
        run: |
          echo "Creating Internet Gateway..."
          cat <<EOF | kubectl apply -f -
          apiVersion: ec2.services.k8s.aws/v1alpha1
          kind: InternetGateway
          metadata:
            name: cluster-$K8S_VERSION_NODOT-igw
          spec:
            tags:
              - key: Name
                value: cluster-$K8S_VERSION_NODOT-igw
            vpcRef:
              from:
                name: cluster-$K8S_VERSION_NODOT
          EOF

      - name: Wait for Internet Gateway Provisioning
        run: |
          echo "Waiting for Internet Gateway to become ready..."
          kubectl wait --for=condition=ACK.ResourceSynced --timeout=300s internetgateway/cluster-$K8S_VERSION_NODOT-igw

      - name: Create Public Route Table
        run: |
          echo "Creating Public Route Table..."
          cat <<EOF | kubectl apply -f -
          apiVersion: ec2.services.k8s.aws/v1alpha1
          kind: RouteTable
          metadata:
            name: cluster-$K8S_VERSION_NODOT-public-rt
          spec:
            vpcRef:
              from:
                name: cluster-$K8S_VERSION_NODOT
            tags:
              - key: Name
                value: cluster-$K8S_VERSION_NODOT-public-rt
            routes:
            - destinationCIDRBlock: 0.0.0.0/0
              gatewayRef:
                from:
                  name: cluster-$K8S_VERSION_NODOT-igw
          EOF

      - name: Wait for Route Table Provisioning
        run: |
          echo "Waiting for Route Table to become ready..."
          kubectl wait --for=condition=ACK.ResourceSynced --timeout=300s routetable/cluster-$K8S_VERSION_NODOT-public-rt

      - name: Deploy Public Subnet-1 CR via EC2 Controller
        run: |
          echo "Deploying Public Subnet-1 resource with Kubernetes version ${{ matrix.k8s_version }}..."
          cat <<EOF | kubectl apply -f -
          apiVersion: ec2.services.k8s.aws/v1alpha1
          kind: Subnet
          metadata:
            name: cluster-$K8S_VERSION_NODOT-public-1
          spec:
            cidrBlock: 10.0.1.0/24
            availabilityZone: eu-central-1a
            mapPublicIPOnLaunch: true
            vpcRef: 
              from: 
                name: cluster-$K8S_VERSION_NODOT
          EOF

      - name: Wait for Public Subnet-1 Provisioning
        run: |
          echo "Waiting for Public Subnet-1 resource to become Ready..."
          kubectl wait --for=condition=ACK.ResourceSynced --timeout=600s subnet/cluster-$K8S_VERSION_NODOT-public-1

      - name: Check Public Subnet-1 status
        if: always()
        run: |
          kubectl get subnet/cluster-$K8S_VERSION_NODOT-public-1 -o yaml

      - name: Deploy Public Subnet-2 CR via EC2 Controller
        run: |
          echo "Deploying Public Subnet-2 resource with Kubernetes version ${{ matrix.k8s_version }}..."
          cat <<EOF | kubectl apply -f -
          apiVersion: ec2.services.k8s.aws/v1alpha1
          kind: Subnet
          metadata:
            name: cluster-$K8S_VERSION_NODOT-public-2
          spec:
            cidrBlock: 10.0.2.0/24
            availabilityZone: eu-central-1b
            mapPublicIPOnLaunch: true
            vpcRef: 
              from: 
                name: cluster-$K8S_VERSION_NODOT
          EOF

      - name: Wait for Public Subnet-2 Provisioning
        run: |
          echo "Waiting for Public Subnet-2 resource to become Ready..."
          kubectl wait --for=condition=ACK.ResourceSynced --timeout=600s subnet/cluster-$K8S_VERSION_NODOT-public-2

      - name: Check Public Subnet-2 status
        if: always()
        run: |
          kubectl get subnet/cluster-$K8S_VERSION_NODOT-public-2 -o yaml

      # - name: Create Security Group for EKS Control Plane
      #   run: |
      #     echo "Creating Security Group for EKS Control Plane..."
      #     cat <<EOF | kubectl apply -f -
      #     apiVersion: ec2.services.k8s.aws/v1alpha1
      #     kind: SecurityGroup
      #     metadata:
      #       name: cluster-$K8S_VERSION_NODOT-eks-sg
      #     spec:
      #       vpcRef:
      #         from:
      #           name: cluster-$K8S_VERSION_NODOT
      #       description: Security group for EKS Control Plane
      #       name: cluster-$K8S_VERSION_NODOT-eks-sg
      #       ingressRules:
      #         - ipProtocol: tcp
      #           fromPort: 443
      #           toPort: 443
      #           ipRanges:
      #             - cidrIP: 0.0.0.0/0
      #               description: Allow HTTPS from anywhere
      #       egressRules:
      #         - ipProtocol: "-1"
      #           fromPort: -1
      #           toPort: -1
      #           ipRanges:
      #             - cidrIP: 0.0.0.0/0
      #               description: Allow all outbound traffic
      #       tags:
      #         - key: Name
      #           value: cluster-$K8S_VERSION_NODOT-eks-sg
      #     EOF

      # - name: Wait for EKS Security Group Provisioning
      #   run: |
      #     echo "Waiting for EKS Security Group to become ready..."
      #     kubectl wait --for=condition=ACK.ResourceSynced --timeout=300s securitygroup/cluster-$K8S_VERSION_NODOT-eks-sg

      # - name: Create Security Group for Worker Nodes
      #   run: |
      #     echo "Creating Security Group for EKS Worker Nodes..."
      #     cat <<EOF | kubectl apply -f -
      #     apiVersion: ec2.services.k8s.aws/v1alpha1
      #     kind: SecurityGroup
      #     metadata:
      #       name: cluster-$K8S_VERSION_NODOT-node-sg
      #     spec:
      #       vpcRef:
      #         from:
      #           name: cluster-$K8S_VERSION_NODOT
      #       description: Security group for EKS Worker Nodes
      #       name: cluster-$K8S_VERSION_NODOT-node-sg
      #       ingressRules:
      #         - ipProtocol: tcp
      #           fromPort: 1025
      #           toPort: 65535
      #           ipRanges:
      #             - cidrIP: 10.0.0.0/16
      #               description: Allow all traffic from within VPC
      #         - ipProtocol: tcp
      #           fromPort: 443
      #           toPort: 443
      #           ipRanges:
      #             - cidrIP: 10.0.0.0/16
      #               description: Allow HTTPS from VPC
      #         - ipProtocol: tcp
      #           fromPort: 22
      #           toPort: 22
      #           ipRanges:
      #             - cidrIP: 0.0.0.0/0
      #               description: Allow SSH from anywhere
      #       egressRules:
      #         - ipProtocol: "-1"
      #           fromPort: -1
      #           toPort: -1
      #           ipRanges:
      #             - cidrIP: 0.0.0.0/0
      #               description: Allow all outbound traffic
      #       tags:
      #         - key: Name
      #           value: cluster-$K8S_VERSION_NODOT-node-sg
      #         - key: kubernetes.io/cluster/cluster-$K8S_VERSION_NODOT
      #           value: owned
      #     EOF

      # - name: Wait for Node Security Group Provisioning
      #   run: |
      #     echo "Waiting for Node Security Group to become ready..."
      #     kubectl wait --for=condition=ACK.ResourceSynced --timeout=300s securitygroup/cluster-$K8S_VERSION_NODOT-node-sg

      # - name: Create Security Group for VPC Endpoints
      #   run: |
      #     echo "Creating Security Group for VPC Endpoints..."
      #     cat <<EOF | kubectl apply -f -
      #     apiVersion: ec2.services.k8s.aws/v1alpha1
      #     kind: SecurityGroup
      #     metadata:
      #       name: cluster-$K8S_VERSION_NODOT-vpce-sg
      #     spec:
      #       vpcRef:
      #         from:
      #           name: cluster-$K8S_VERSION_NODOT
      #       description: Security group for VPC Endpoints
      #       name: cluster-$K8S_VERSION_NODOT-vpce-sg
      #       ingressRules:
      #         - ipProtocol: tcp
      #           fromPort: 443
      #           toPort: 443
      #           ipRanges:
      #             - cidrIP: 10.0.0.0/16
      #               description: Allow HTTPS from VPC CIDR
      #       egressRules:
      #         - ipProtocol: "-1"
      #           fromPort: -1
      #           toPort: -1
      #           ipRanges:
      #             - cidrIP: 0.0.0.0/0
      #               description: Allow all outbound traffic
      #       tags:
      #         - key: Name
      #           value: cluster-$K8S_VERSION_NODOT-vpce-sg
      #     EOF

      # - name: Wait for VPC Endpoints Security Group Provisioning
      #   run: |
      #     echo "Waiting for VPC Endpoints Security Group to become ready..."
      #     kubectl wait --for=condition=ACK.ResourceSynced --timeout=300s securitygroup/cluster-$K8S_VERSION_NODOT-vpce-sg

      # - name: Deploy VPC Endpoint for EC2
      #   run: |
      #     echo "Deploying VPC Endpoint for EC2..."
      #     cat <<EOF | kubectl apply -f -
      #     apiVersion: ec2.services.k8s.aws/v1alpha1
      #     kind: VPCEndpoint
      #     metadata:
      #       name: cluster-$K8S_VERSION_NODOT-ec2-endpoint
      #     spec:
      #       serviceName: com.amazonaws.eu-central-1.ec2
      #       vpcRef:
      #         from:
      #           name: cluster-$K8S_VERSION_NODOT
      #       subnetRefs:
      #         - from:
      #             name: cluster-$K8S_VERSION_NODOT-public-1
      #         - from:
      #             name: cluster-$K8S_VERSION_NODOT-public-2
      #       securityGroupRefs:
      #         - from:
      #             name: cluster-$K8S_VERSION_NODOT-vpce-sg
      #       vpcEndpointType: Interface
      #       privateDNSEnabled: true
      #       policyDocument: |
      #         {
      #           "Version": "2012-10-17",
      #           "Statement": [
      #             {
      #               "Effect": "Allow",
      #               "Principal": "*",
      #               "Action": "*",
      #               "Resource": "*"
      #             }
      #           ]
      #         }
      #       tags:
      #         - key: Name
      #           value: cluster-$K8S_VERSION_NODOT-ec2-endpoint
      #     EOF

      # - name: Deploy VPC Endpoint for ECR API
      #   run: |
      #     echo "Deploying VPC Endpoint for ECR API..."
      #     cat <<EOF | kubectl apply -f -
      #     apiVersion: ec2.services.k8s.aws/v1alpha1
      #     kind: VPCEndpoint
      #     metadata:
      #       name: cluster-$K8S_VERSION_NODOT-ecr-api-endpoint
      #     spec:
      #       serviceName: com.amazonaws.eu-central-1.ecr.api
      #       vpcRef:
      #         from:
      #           name: cluster-$K8S_VERSION_NODOT
      #       subnetRefs:
      #         - from:
      #             name: cluster-$K8S_VERSION_NODOT-public-1
      #         - from:
      #             name: cluster-$K8S_VERSION_NODOT-public-2
      #       securityGroupRefs:
      #         - from:
      #             name: cluster-$K8S_VERSION_NODOT-vpce-sg
      #       vpcEndpointType: Interface
      #       privateDNSEnabled: true
      #       policyDocument: |
      #         {
      #           "Version": "2012-10-17",
      #           "Statement": [
      #             {
      #               "Effect": "Allow",
      #               "Principal": "*",
      #               "Action": "*",
      #               "Resource": "*"
      #             }
      #           ]
      #         }
      #       tags:
      #         - key: Name
      #           value: cluster-$K8S_VERSION_NODOT-ecr-api-endpoint
      #     EOF

      # - name: Deploy VPC Endpoint for ECR Docker
      #   run: |
      #     echo "Deploying VPC Endpoint for ECR Docker..."
      #     cat <<EOF | kubectl apply -f -
      #     apiVersion: ec2.services.k8s.aws/v1alpha1
      #     kind: VPCEndpoint
      #     metadata:
      #       name: cluster-$K8S_VERSION_NODOT-ecr-dkr-endpoint
      #     spec:
      #       serviceName: com.amazonaws.eu-central-1.ecr.dkr
      #       vpcRef:
      #         from:
      #           name: cluster-$K8S_VERSION_NODOT
      #       subnetRefs:
      #         - from:
      #             name: cluster-$K8S_VERSION_NODOT-public-1
      #         - from:
      #             name: cluster-$K8S_VERSION_NODOT-public-2
      #       securityGroupRefs:
      #         - from:
      #             name: cluster-$K8S_VERSION_NODOT-vpce-sg
      #       vpcEndpointType: Interface
      #       privateDNSEnabled: true
      #       policyDocument: |
      #         {
      #           "Version": "2012-10-17",
      #           "Statement": [
      #             {
      #               "Effect": "Allow",
      #               "Principal": "*",
      #               "Action": "*",
      #               "Resource": "*"
      #             }
      #           ]
      #         }
      #       tags:
      #         - key: Name
      #           value: cluster-$K8S_VERSION_NODOT-ecr-dkr-endpoint
      #     EOF

      # - name: Deploy VPC Endpoint for STS
      #   run: |
      #     echo "Deploying VPC Endpoint for STS..."
      #     cat <<EOF | kubectl apply -f -
      #     apiVersion: ec2.services.k8s.aws/v1alpha1
      #     kind: VPCEndpoint
      #     metadata:
      #       name: cluster-$K8S_VERSION_NODOT-sts-endpoint
      #     spec:
      #       serviceName: com.amazonaws.eu-central-1.sts
      #       vpcRef:
      #         from:
      #           name: cluster-$K8S_VERSION_NODOT
      #       subnetRefs:
      #         - from:
      #             name: cluster-$K8S_VERSION_NODOT-public-1
      #         - from:
      #             name: cluster-$K8S_VERSION_NODOT-public-2
      #       securityGroupRefs:
      #         - from:
      #             name: cluster-$K8S_VERSION_NODOT-vpce-sg
      #       vpcEndpointType: Interface
      #       privateDNSEnabled: true
      #       policyDocument: |
      #         {
      #           "Version": "2012-10-17",
      #           "Statement": [
      #             {
      #               "Effect": "Allow",
      #               "Principal": "*",
      #               "Action": "*",
      #               "Resource": "*"
      #             }
      #           ]
      #         }
      #       tags:
      #         - key: Name
      #           value: cluster-$K8S_VERSION_NODOT-sts-endpoint
      #     EOF

      # Add a wait step for VPC Endpoints
      #   - name: Wait for VPC Endpoints Provisioning
      #     run: |
      #       echo "Waiting for VPC Endpoints to become Ready..."
      #       kubectl wait --for=condition=ACK.ResourceSynced --timeout=600s vpcendpoint/cluster-$K8S_VERSION_NODOT-ec2-endpoint
      #       kubectl wait --for=condition=ACK.ResourceSynced --timeout=600s vpcendpoint/cluster-$K8S_VERSION_NODOT-ecr-api-endpoint
      #       kubectl wait --for=condition=ACK.ResourceSynced --timeout=600s vpcendpoint/cluster-$K8S_VERSION_NODOT-ecr-dkr-endpoint
      #       kubectl wait --for=condition=ACK.ResourceSynced --timeout=600s vpcendpoint/cluster-$K8S_VERSION_NODOT-sts-endpoint

      - name: Deploy Cluster CR via EKS Controller
        run: |
          echo "Deploying EKS Cluster resource with Kubernetes version ${{ matrix.k8s_version }}..."
          cat <<EOF | kubectl apply -f -
          apiVersion: eks.services.k8s.aws/v1alpha1
          kind: Cluster
          metadata:
            name: cluster-$K8S_VERSION_NODOT
          spec:
            name: cluster-$K8S_VERSION_NODOT
            roleARN: arn:aws:iam::654654440307:role/EKSClusterRole
            version: "${{ matrix.k8s_version }}"
            resourcesVPCConfig:
              endpointPrivateAccess: true
              endpointPublicAccess: true
              subnetRefs:
              - from:
                  name: cluster-$K8S_VERSION_NODOT-public-1
              - from:
                  name: cluster-$K8S_VERSION_NODOT-public-2
          EOF

      - name: Wait for Cluster Provisioning
        run: |
          echo "Waiting for EKS Cluster resource to become Ready..."
          kubectl wait --for=condition=ACK.ResourceSynced --timeout=900s cluster/cluster-$K8S_VERSION_NODOT

      - name: Check Cluster status
        if: always()
        run: |
          kubectl get cluster/cluster-$K8S_VERSION_NODOT -o yaml

      - name: Deploy Nodegroup CR via EKS Controller
        run: |
          echo "Deploying EKS Nodegroup resource with Kubernetes version ${{ matrix.k8s_version }}..."
          cat <<EOF | kubectl apply -f -
          apiVersion: eks.services.k8s.aws/v1alpha1
          kind: Nodegroup
          metadata:
            name: cluster-$K8S_VERSION_NODOT
          spec:
            version: "${{ matrix.k8s_version }}"
            name: cluster-$K8S_VERSION_NODOT
            clusterRef:
              from:
                name: cluster-$K8S_VERSION_NODOT       
            nodeRole: arn:aws:iam::654654440307:role/eks-node-role
            subnetRefs:
              - from:
                  name: cluster-$K8S_VERSION_NODOT-public-1
              - from:
                  name: cluster-$K8S_VERSION_NODOT-public-2
            scalingConfig:
              minSize: 2
              maxSize: 5
              desiredSize: 3
          EOF

      - name: Wait for Nodegroup Provisioning
        run: |
          echo "Waiting for EKS Nodegroup resource to become Ready..."
          kubectl wait nodegroup/cluster-$K8S_VERSION_NODOT --for=jsonpath='{.status.status}'=CREATE_FAILED --timeout=3600s

      - name: Check Nodegroup status
        if: always()
        run: |
          kubectl get nodegroup/cluster-$K8S_VERSION_NODOT -o yaml

      #  - name: Get kubeconfig
      #    run: aws eks update-kubeconfig --region eu-central-1 --name cluster-${K8S_VERSION_NODOT} --profile dev

      - name: Get events
        if: always()
        run: kubectl get events -A --sort-by='.lastTimestamp'

      - name: describe-nodegroup
        if: always()
        run: |
          aws eks describe-nodegroup --region eu-central-1 --cluster-name cluster-$K8S_VERSION_NODOT --nodegroup-name cluster-$K8S_VERSION_NODOT

      - name: Debug EKS nodegroup (describe + health issues)
        shell: bash
        run: |
          set -euo pipefail

          CLUSTER="cluster-${K8S_VERSION_NODOT}"
          NODEGROUP="cluster-${K8S_VERSION_NODOT}"
          REGION="eu-central-1"

          echo "Cluster:   $CLUSTER"
          echo "Nodegroup: $NODEGROUP"
          echo "Region:    $REGION"
          echo

          echo "== aws eks describe-nodegroup (full) =="
          aws eks describe-nodegroup \
            --region "$REGION" \
            --cluster-name "$CLUSTER" \
            --nodegroup-name "$NODEGROUP" \
            --output json | tee describe-nodegroup.json
          echo

          echo "== Nodegroup health issues (table) =="
          aws eks describe-nodegroup \
            --region "$REGION" \
            --cluster-name "$CLUSTER" \
            --nodegroup-name "$NODEGROUP" \
            --query 'nodegroup.health.issues[*].{code:code,message:message,resourceIds:resourceIds}' \
            --output table || true
          echo

          echo "== Nodegroup status summary =="
          aws eks describe-nodegroup \
            --region "$REGION" \
            --cluster-name "$CLUSTER" \
            --nodegroup-name "$NODEGROUP" \
            --query 'nodegroup.{status:status,version:version,releaseVersion:releaseVersion,createdAt:createdAt,modifiedAt:modifiedAt}' \
            --output table
          echo

          echo "== Extract ASG name =="
          ASG="$(aws eks describe-nodegroup \
            --region "$REGION" \
            --cluster-name "$CLUSTER" \
            --nodegroup-name "$NODEGROUP" \
            --query 'nodegroup.resources.autoScalingGroups[0].name' \
            --output text)"
          echo "ASG: $ASG"
          echo "ASG=$ASG" >> "$GITHUB_ENV"
          echo

          echo "== Extract instance IDs from health issues (if present) =="
          INSTANCE_IDS="$(aws eks describe-nodegroup \
            --region "$REGION" \
            --cluster-name "$CLUSTER" \
            --nodegroup-name "$NODEGROUP" \
            --query 'nodegroup.health.issues[?code==`NodeCreationFailure`].resourceIds[]' \
            --output text || true)"
          echo "INSTANCE_IDS: ${INSTANCE_IDS:-<none>}"
          echo "INSTANCE_IDS=${INSTANCE_IDS}" >> "$GITHUB_ENV"
          echo

      - name: Debug AutoScaling activity (ASG scaling activities)
        shell: bash
        run: |
          set -euo pipefail
          REGION="eu-central-1"

          if [[ -z "${ASG:-}" ]]; then
            echo "ASG not set; skipping."
            exit 0
          fi

          echo "== autoscaling describe-scaling-activities (last 20) =="
          aws autoscaling describe-scaling-activities \
            --region "$REGION" \
            --auto-scaling-group-name "$ASG" \
            --max-items 20 \
            --query 'Activities[*].[StartTime,StatusCode,Description,Cause]' \
            --output table || true
          echo

      - name: Debug one EC2 instance (console output + describe)
        shell: bash
        run: |
          set -euo pipefail
          REGION="eu-central-1"

          if [[ -z "${INSTANCE_IDS:-}" ]]; then
            echo "No instance IDs found in nodegroup health issues; skipping."
            exit 0
          fi

          # Pick the first instance ID
          FIRST_INSTANCE="$(echo "$INSTANCE_IDS" | awk '{print $1}')"
          echo "Using instance: $FIRST_INSTANCE"
          echo "FIRST_INSTANCE=$FIRST_INSTANCE" >> "$GITHUB_ENV"
          echo

          echo "== ec2 describe-instances (summary) =="
          aws ec2 describe-instances \
            --region "$REGION" \
            --instance-ids "$FIRST_INSTANCE" \
            --query 'Reservations[0].Instances[0].{State:State.Name,PrivateIp:PrivateIpAddress,SubnetId:SubnetId,VpcId:VpcId,SGs:SecurityGroups[*].GroupId,LaunchTime:LaunchTime,ImageId:ImageId,InstanceType:InstanceType}' \
            --output table || true
          echo

          echo "== ec2 get-console-output (latest) =="
          # This is where bootstrap/cloud-init/kubelet join errors often appear
          aws ec2 get-console-output \
            --region "$REGION" \
            --instance-id "$FIRST_INSTANCE" \
            --latest \
            --output text || true
          echo

      - name: Debug EKS cluster endpoint access mode
        shell: bash
        run: |
          set -euo pipefail

          CLUSTER="cluster-${K8S_VERSION_NODOT}"
          REGION="eu-central-1"

          echo "== eks describe-cluster endpoint flags =="
          aws eks describe-cluster \
            --region "$REGION" \
            --name "$CLUSTER" \
            --query 'cluster.resourcesVpcConfig.{endpointPublicAccess:endpointPublicAccess,endpointPrivateAccess:endpointPrivateAccess,publicAccessCidrs:publicAccessCidrs}' \
            --output table
          echo

          echo "== eks describe-cluster control-plane security groups & subnets =="
          aws eks describe-cluster \
            --region "$REGION" \
            --name "$CLUSTER" \
            --query 'cluster.resourcesVpcConfig.{subnetIds:subnetIds,securityGroupIds:securityGroupIds,clusterSecurityGroupId:clusterSecurityGroupId,vpcId:vpcId}' \
            --output json
          echo

      - name: Debug nodegroup subnets route tables (IGW/NAT check)
        shell: bash
        run: |
          set -euo pipefail

          CLUSTER="cluster-${K8S_VERSION_NODOT}"
          NODEGROUP="cluster-${K8S_VERSION_NODOT}"
          REGION="eu-central-1"

          echo "== Get nodegroup subnet IDs =="
          SUBNETS="$(aws eks describe-nodegroup \
            --region "$REGION" \
            --cluster-name "$CLUSTER" \
            --nodegroup-name "$NODEGROUP" \
            --query 'nodegroup.subnets' \
            --output text || true)"
          echo "Subnets: ${SUBNETS:-<none>}"
          echo

          if [[ -z "${SUBNETS:-}" ]]; then
            echo "No subnets found; skipping."
            exit 0
          fi

          echo "== Describe route tables associated to these subnets =="
          # This helps spot "private subnet without NAT" and similar routing issues
          aws ec2 describe-route-tables \
            --region "$REGION" \
            --filters "Name=association.subnet-id,Values=${SUBNETS// /,}" \
            --query 'RouteTables[*].{rtb:RouteTableId,associations:Associations[*].SubnetId,routes:Routes[*].{dest:DestinationCidrBlock,gw:GatewayId,nat:NatGatewayId,tgw:TransitGatewayId,eni:NetworkInterfaceId}}' \
            --output json || true
          echo

      - name: Optional: Check EKS access entries (new auth path)
        shell: bash
        run: |
          set -euo pipefail

          CLUSTER="cluster-${K8S_VERSION_NODOT}"
          REGION="eu-central-1"

          echo "== eks list-access-entries =="
          aws eks list-access-entries \
            --region "$REGION" \
            --cluster-name "$CLUSTER" \
            --output json || true
          echo

      - name: Upload debug artifacts (describe-nodegroup.json)
        uses: actions/upload-artifact@v4
        with:
          name: eks-nodegroup-debug
          path: |
            describe-nodegroup.json

      - name: Clean Up Sensitive Files
        if: always()
        run: rm -f certificato.txt chiave_privata_decifrata.key ./credentials

      - name: Clean Up Nodegroup Resource
        if: always()
        run: kubectl delete nodegroup/cluster-$K8S_VERSION_NODOT

      - name: Clean Up Cluster Resource
        if: always()
        run: kubectl delete cluster/cluster-$K8S_VERSION_NODOT

      - name: Clean Up VPC Endpoints Resources
        if: always()
        run: |
          kubectl delete vpcendpoint/cluster-$K8S_VERSION_NODOT-ec2-endpoint
          kubectl delete vpcendpoint/cluster-$K8S_VERSION_NODOT-ecr-api-endpoint
          kubectl delete vpcendpoint/cluster-$K8S_VERSION_NODOT-ecr-dkr-endpoint
          kubectl delete vpcendpoint/cluster-$K8S_VERSION_NODOT-sts-endpoint

      - name: Clean Up VPC Endpoints Security Group Resource
        if: always()
        run: kubectl delete securitygroup/cluster-$K8S_VERSION_NODOT-vpce-sg

      - name: Celan Up Node Security Group Resource
        if: always()
        run: kubectl delete securitygroup/cluster-$K8S_VERSION_NODOT-node-sg

      - name: Clean Up EKS Security Group Resource
        if: always()
        run: kubectl delete securitygroup/cluster-$K8S_VERSION_NODOT-eks-sg

      - name: Clean Up Public Subnet-2 Resource
        if: always()
        run: kubectl delete subnet/cluster-$K8S_VERSION_NODOT-public-2

      - name: Clean Up Public Subnet-1 Resource
        if: always()
        run: kubectl delete subnet/cluster-$K8S_VERSION_NODOT-public-1

      - name: Clean Up Route Table Resource
        if: always()
        run: kubectl delete routetable/cluster-$K8S_VERSION_NODOT-public-rt

      - name: Clean Up Internet Gateway Resource
        if: always()
        run: kubectl delete internetgateway/cluster-$K8S_VERSION_NODOT-igw

      - name: Clean Up VPC Resource
        if: always()
        run: kubectl delete vpc/cluster-$K8S_VERSION_NODOT